#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —á–∞—Å—Ç—è–º
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent))

from parser.ru_nlq import NLQParser

def debug_parts():
    """–û—Ç–ª–∞–¥–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —á–∞—Å—Ç—è–º"""
    
    print("üîç –û—Ç–ª–∞–¥–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —á–∞—Å—Ç—è–º")
    print("=" * 60)
    
    parser = NLQParser()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    query = "—É—Å–ª—É–≥–∏ –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É –¥–æ—Ä–æ–≥ –ö—Ä—ã–º –æ—Ç 5 –¥–æ 20 –º–ª–Ω"
    lemmatized = parser._lemmatize_text(parser._normalize_text(query))
    
    print(f"üìù –ó–∞–ø—Ä–æ—Å: {query}")
    print(f"üìù –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {lemmatized}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç—è–º
    print(f"\nüîç –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç—è–º:")
    query_words = set(lemmatized.split())
    print(f"   –°–ª–æ–≤–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ: {query_words}")
    
    for keyword, okpd2_codes in parser.okpd2_dict.items():
        keyword_words = keyword.split()
        print(f"\n   –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{keyword}' -> {okpd2_codes}")
        print(f"   –°–ª–æ–≤–∞ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞: {keyword_words}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
        all_found = True
        for word in keyword_words:
            if word in query_words:
                print(f"      ‚úÖ '{word}' –Ω–∞–π–¥–µ–Ω–æ –≤ –∑–∞–ø—Ä–æ—Å–µ")
            else:
                print(f"      ‚ùå '{word}' –ù–ï –Ω–∞–π–¥–µ–Ω–æ –≤ –∑–∞–ø—Ä–æ—Å–µ")
                all_found = False
        
        if all_found:
            print(f"      üéØ –í–°–ï —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã! –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ '{keyword}' –ø–æ–¥—Ö–æ–¥–∏—Ç")
        else:
            print(f"      ‚ö†Ô∏è  –ù–µ –≤—Å–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –¥–æ—Ä–æ–≥"
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –¥–æ—Ä–æ–≥':")
    keyword = "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –¥–æ—Ä–æ–≥"
    if keyword in parser.okpd2_dict:
        keyword_words = keyword.split()
        print(f"   –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ '{keyword}' –µ—Å—Ç—å –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ")
        print(f"   –°–ª–æ–≤–∞: {keyword_words}")
        print(f"   –ï—Å—Ç—å –ª–∏ '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ' –≤ –∑–∞–ø—Ä–æ—Å–µ: {'—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ' in query_words}")
        print(f"   –ï—Å—Ç—å –ª–∏ '–¥–æ—Ä–æ–≥' –≤ –∑–∞–ø—Ä–æ—Å–µ: {'–¥–æ—Ä–æ–≥' in query_words}")
        
        if all(word in query_words for word in keyword_words):
            print(f"   ‚úÖ –í–°–ï —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã!")
        else:
            print(f"   ‚ùå –ù–µ –≤—Å–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã")
    else:
        print(f"   ‚ùå –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ '{keyword}' –ù–ï –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ")

if __name__ == "__main__":
    debug_parts()
