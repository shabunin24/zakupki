#!/usr/bin/env python3
"""
ÐžÑ‚Ð»Ð°Ð´ÐºÐ° Ð¿Ð°Ñ€ÑÐµÑ€Ð° ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°
"""

import sys
from pathlib import Path

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð² Ð¿ÑƒÑ‚ÑŒ Ð´Ð»Ñ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð°
sys.path.insert(0, str(Path(__file__).parent))

from parser.ru_nlq import NLQParser

def debug_parser():
    """ÐžÑ‚Ð»Ð°Ð´ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¿Ð°Ñ€ÑÐµÑ€Ð°"""
    
    print("ðŸ” ÐžÑ‚Ð»Ð°Ð´ÐºÐ° Ð¿Ð°Ñ€ÑÐµÑ€Ð° ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°")
    print("=" * 60)
    
    parser = NLQParser()
    
    # Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
    query = "Ð±ÑƒÐ¼Ð°Ð³Ð° Ð4 Ð¾Ñ„Ð¸ÑÐ½Ð°Ñ ÐœÐ¾ÑÐºÐ²Ð° Ð´Ð¾ 200 Ñ‚Ñ‹Ñ Ñ€ÑƒÐ±"
    print(f"ðŸ“ Ð—Ð°Ð¿Ñ€Ð¾Ñ: {query}")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ
    normalized = parser._normalize_text(query)
    print(f"ðŸ“ ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹: {normalized}")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð»ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸ÑŽ
    lemmatized = parser._lemmatize_text(normalized)
    print(f"ðŸ“ Ð›ÐµÐ¼Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹: {lemmatized}")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
    print(f"\nðŸ“š Ð¡Ð¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸Ðº ÐžÐšÐŸÐ”2 (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5 ÐºÐ»ÑŽÑ‡ÐµÐ¹):")
    for i, (key, value) in enumerate(parser.okpd2_dict.items()):
        if i >= 5:
            break
        print(f"   {key}: {value}")
    
    print(f"\nðŸ“š Ð¡Ð¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¸Ðº Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð¾Ð² (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5):")
    for i, region in enumerate(parser.regions_dict):
        if i >= 5:
            break
        print(f"   {region['canonical']}: {region['aliases']}")
    
    # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ
    print(f"\nðŸ” Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°:")
    result = parser.parse(query)
    
    print(f"   ÐžÐšÐŸÐ”2: {result['okpd2']}")
    print(f"   Ð ÐµÐ³Ð¸Ð¾Ð½: {result['region']}")
    print(f"   Ð¦ÐµÐ½Ð° Ð´Ð¾: {result['price_max']}")
    print(f"   Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°: {result['diagnostics']}")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿Ð¾Ð¸ÑÐº ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð²
    print(f"\nðŸ” ÐŸÐ¾Ð¸ÑÐº ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð²:")
    for keyword, okpd2_codes in parser.okpd2_dict.items():
        if keyword in lemmatized:
            print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾: {keyword} -> {okpd2_codes}")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿Ð¾Ð¸ÑÐº Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð¾Ð²
    print(f"\nðŸ” ÐŸÐ¾Ð¸ÑÐº Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð¾Ð²:")
    for region_info in parser.regions_dict:
        canonical = region_info["canonical"]
        aliases = region_info["aliases"]
        
        if canonical.lower() in lemmatized:
            print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÐºÐ°Ð½Ð¾Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ: {canonical}")
        
        for alias in aliases:
            if alias.lower() in lemmatized:
                print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ð¾ ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ñƒ: {alias} -> {canonical}")

if __name__ == "__main__":
    debug_parser()
